{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12588887,"sourceType":"datasetVersion","datasetId":7950899},{"sourceId":13040790,"sourceType":"datasetVersion","datasetId":8257759},{"sourceId":13041611,"sourceType":"datasetVersion","datasetId":8258322},{"sourceId":13085633,"sourceType":"datasetVersion","datasetId":8288147},{"sourceId":596115,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":446358,"modelId":462819}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow opencv-python numpy matplotlib tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:49:11.982996Z","iopub.execute_input":"2025-10-02T16:49:11.983765Z","iopub.status.idle":"2025-10-02T16:49:15.181222Z","shell.execute_reply.started":"2025-10-02T16:49:11.983740Z","shell.execute_reply":"2025-10-02T16:49:15.180304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Define the folder path and the file path\nnew_folder = \"/kaggle/working/3DCAE\"\n# new_file_path = os.path.join(new_folder, \"vid2array.txt\")\n\n# Create the new folder. The exist_ok=True flag prevents errors if it already exists.\nos.makedirs(new_folder, exist_ok=True)\nprint(f\"Directory '{new_folder}' created successfully (or already exists).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:49:26.959547Z","iopub.execute_input":"2025-10-02T16:49:26.959811Z","iopub.status.idle":"2025-10-02T16:49:26.964464Z","shell.execute_reply.started":"2025-10-02T16:49:26.959791Z","shell.execute_reply":"2025-10-02T16:49:26.963657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/3DCAE/vid2array.py\nimport os\nimport random\n\n# --- Configuration for both datasets ---\n\n# Avenue Dataset Paths\nAVENUE_TRAIN_VIDEOS = \"/kaggle/input/avenuedataset/Avenuedataset/Avenue_Dataset/Avenue Dataset/training_videos\"\nAVENUE_TEST_VIDEOS = \"/kaggle/input/avenuedataset/Avenuedataset/Avenue_Dataset/Avenue Dataset/testing_videos\"\n\n# FUTD (FUTMINNA) Dataset Paths\nFUTD_TRAIN_VIDEOS = \"/kaggle/input/futminna-dataset/training_videos/training_videos\"\nFUTD_TEST_VIDEOS = \"/kaggle/input/futminna-dataset/testing_videos/testing_videos\"\n\nARMY_VIDEOS = \"/kaggle/input/armybandit-dataset\"\nBANDIT_TEST_VIDEOS = \"/kaggle/input/bandit-dataset\"\n\n# Output Paths for Combined Datasets\nOUTPUT_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nTRAINING_VIDEO_PATHS_FILE = os.path.join(OUTPUT_DIR, \"combined_training_video_paths.txt\")\nTESTING_VIDEO_PATHS_FILE = os.path.join(OUTPUT_DIR, \"combined_testing_video_paths.txt\")\nVALIDATION_VIDEO_PATHS_FILE = os.path.join(OUTPUT_DIR, \"combined_validation_video_paths.txt\")\n\n# --- Helper Function to Get Video Paths ---\ndef get_video_paths_from_dir(base_dir):\n    \"\"\"\n    Scans a directory and returns a list of video file paths.\n    \"\"\"\n    video_paths = []\n    # Check if the directory exists and is not empty\n    if not os.path.exists(base_dir):\n        print(f\"Warning: Directory not found: {base_dir}\")\n        return []\n\n    # Get all video files (assuming mp4, avi, etc.)\n    for root, dirs, files in os.walk(base_dir):\n        for file in files:\n            if file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n                video_paths.append(os.path.join(root, file))\n    return video_paths\n\n# --- Main Script to Generate Combined Path Files ---\ndef generate_combined_paths():\n    \"\"\"\n    Combines video paths from multiple datasets and splits them into\n    training, validation, and testing sets.\n    \"\"\"\n    print(\"Combining Avenue and FUTD training video paths...\")\n    avenue_train_paths = get_video_paths_from_dir(AVENUE_TRAIN_VIDEOS)\n    futd_train_paths = get_video_paths_from_dir(FUTD_TRAIN_VIDEOS)\n    army_train_paths = get_video_paths_from_dir(ARMY_VIDEOS)\n    \n    # Simple aggregation for training\n    all_training_paths = avenue_train_paths + futd_train_paths + army_train_paths\n    random.shuffle(all_training_paths) # Shuffle to mix the two datasets\n    \n    # Split training paths into a smaller validation set (e.g., 20%)\n    validation_split_index = int(len(all_training_paths) * 0.2)\n    training_paths = all_training_paths[validation_split_index:]\n    validation_paths = all_training_paths[:validation_split_index]\n    \n    print(f\"Total training videos: {len(training_paths)}\")\n    print(f\"Total validation videos: {len(validation_paths)}\")\n\n    # Combine testing paths\n    print(\"\\nCombining Avenue, army and FUTD testing video paths...\")\n    avenue_test_paths = get_video_paths_from_dir(AVENUE_TEST_VIDEOS)\n    futd_test_paths = get_video_paths_from_dir(FUTD_TEST_VIDEOS)\n    bandit_test_path = get_video_paths_from_dir(BANDIT_TEST_VIDEOS)\n    \n    all_testing_paths = avenue_test_paths + futd_test_paths + bandit_test_path\n    print(f\"Total testing videos: {len(all_testing_paths)}\")\n\n    # Write paths to files\n    with open(TRAINING_VIDEO_PATHS_FILE, 'w') as f:\n        for path in training_paths:\n            f.write(f\"{path}\\n\")\n\n    with open(VALIDATION_VIDEO_PATHS_FILE, 'w') as f:\n        for path in validation_paths:\n            f.write(f\"{path}\\n\")\n    \n    with open(TESTING_VIDEO_PATHS_FILE, 'w') as f:\n        for path in all_testing_paths:\n            f.write(f\"{path}\\n\")\n\n    print(f\"\\nSuccessfully generated combined path files in {OUTPUT_DIR}:\")\n    print(f\"- {os.path.basename(TRAINING_VIDEO_PATHS_FILE)}\")\n    print(f\"- {os.path.basename(VALIDATION_VIDEO_PATHS_FILE)}\")\n    print(f\"- {os.path.basename(TESTING_VIDEO_PATHS_FILE)}\")\n\nif __name__ == '__main__':\n    generate_combined_paths()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:18:25.759066Z","iopub.execute_input":"2025-10-09T13:18:25.759965Z","iopub.status.idle":"2025-10-09T13:18:25.766923Z","shell.execute_reply.started":"2025-10-09T13:18:25.759929Z","shell.execute_reply":"2025-10-09T13:18:25.766188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python /kaggle/working/3DCAE/vid2array.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:18:25.958274Z","iopub.execute_input":"2025-10-09T13:18:25.958579Z","iopub.status.idle":"2025-10-09T13:18:26.288042Z","shell.execute_reply.started":"2025-10-09T13:18:25.958557Z","shell.execute_reply":"2025-10-09T13:18:26.287165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/3DCAE/data_generator.py\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nimport os\n\nclass VideoSequenceGenerator(Sequence):\n    def __init__(self, video_paths_file, sequence_length, resize_dim=(128, 128), batch_size=1, shuffle=True): # <--- BATCH_SIZE=1, RESIZE_DIM=(128,128)\n        self.video_paths = self._load_video_paths(video_paths_file)\n        self.sequence_length = sequence_length\n        self.resize_dim = resize_dim\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.on_epoch_end() # Call to shuffle data at the start\n\n    def _load_video_paths(self, file_path):\n        with open(file_path, 'r') as f:\n            paths = [line.strip() for line in f if line.strip()]\n        return paths\n\n    def _get_video_sequences(self, video_path):\n        # This function loads frames from a single video and extracts all possible sequences\n        frames = []\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            return []\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.resize(frame, self.resize_dim)\n            frame = frame / 255.0 # Normalize to [0, 1]\n            frames.append(frame)\n        cap.release()\n        \n        # Extract sequences (e.g., 8 frames each)\n        sequences_from_video = []\n        if len(frames) >= self.sequence_length:\n            for i in range(0, len(frames) - self.sequence_length + 1):\n                sequence = frames[i : i + self.sequence_length]\n                sequences_from_video.append(np.array(sequence, dtype=np.float32))\n        \n        return sequences_from_video\n\n    def __len__(self):\n        return int(np.ceil(len(self.video_paths) / self.batch_size))\n\n\n    def __getitem__(self, idx):\n        batch_video_paths_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        all_sequences_in_batch = []\n        for i in batch_video_paths_indices:\n            video_path = self.video_paths[i]\n            sequences_from_current_video = self._get_video_sequences(video_path)\n            all_sequences_in_batch.extend(sequences_from_current_video)\n\n        if not all_sequences_in_batch:\n            dummy_input = np.zeros((self.batch_size, self.sequence_length, *self.resize_dim, 3), dtype=np.float32)\n            return dummy_input, dummy_input\n\n        np.random.shuffle(all_sequences_in_batch)\n        final_batch_sequences = all_sequences_in_batch[:self.batch_size]\n        \n        if not final_batch_sequences:\n            dummy_input = np.zeros((0, self.sequence_length, *self.resize_dim, 3), dtype=np.float32)\n            return dummy_input, dummy_input\n\n        batch_sequences_array = np.array(final_batch_sequences, dtype=np.float32)\n\n        return batch_sequences_array, batch_sequences_array\n\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.video_paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indices)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:18:31.889305Z","iopub.execute_input":"2025-10-09T13:18:31.890164Z","iopub.status.idle":"2025-10-09T13:18:31.896814Z","shell.execute_reply.started":"2025-10-09T13:18:31.890118Z","shell.execute_reply":"2025-10-09T13:18:31.896196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/3DCAE/ave_gt_conv.py\n# Cell for .mat to .npy conversion (run this!)\n# Cell for .mat to .npy conversion (run this!)\nimport os\nimport scipy.io\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nprint(\"Starting conversion of .mat ground truth files to .npy...\")\n\nINPUT_GT_DIR = \"/kaggle/input/avenuedataset/Avenuedataset/ground_truth_demo/ground_truth_demo/testing_label_mask\"\nOUTPUT_PROCESSED_DATA_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data\"\nOUTPUT_GT_DIR = os.path.join(OUTPUT_PROCESSED_DATA_DIR, \"combined_testing_label_mask_npy\")\nos.makedirs(OUTPUT_GT_DIR, exist_ok=True)\n\nmat_files = [f for f in os.listdir(INPUT_GT_DIR) if f.endswith(\".mat\")]\n\nif not mat_files:\n    print(f\"No .mat files found in {INPUT_GT_DIR}. Please check the ground truth path.\")\nelse:\n    for mat_file in tqdm(mat_files, desc=\"Converting .mat to .npy\"):\n        mat_path = os.path.join(INPUT_GT_DIR, mat_file)\n        \n        try:\n            mat_data = scipy.io.loadmat(mat_path)\n            \n            if 'volLabel' in mat_data: \n                mask_data_raw = mat_data['volLabel'] \n                \n                frame_level_anomaly = None\n                \n                if mask_data_raw.ndim == 2 and mask_data_raw.shape[0] == 1 and mask_data_raw.dtype == object:\n                    list_of_frame_masks = mask_data_raw[0]\n                    frame_level_anomaly = np.array([1 if np.any(frame_mask > 0) else 0 for frame_mask in list_of_frame_masks]).astype(int)\n                elif mask_data_raw.ndim == 1:\n                    frame_level_anomaly = (mask_data_raw.astype(float) > 0).astype(int)\n                elif mask_data_raw.ndim == 3:\n                    frame_level_anomaly = np.array([1 if np.any(mask_data_raw[:, :, i].astype(float) > 0) else 0 for i in range(mask_data_raw.shape[2])])\n                else:\n                    print(f\"Warning: Unexpected 'volLabel' shape or dtype in {mat_file}: {mask_data_raw.shape}, {mask_data_raw.dtype}. Skipping conversion.\")\n                    continue \n                    \n                if frame_level_anomaly is not None:\n                    # Fix: Correctly format the filename with a leading zero\n                    base_name = os.path.splitext(mat_file)[0]\n                    try:\n                        video_id = int(base_name.split('_')[0])\n                        npy_filename = f\"{video_id:02d}_label.npy\"\n                    except (ValueError, IndexError):\n                        # Fallback for unexpected naming, though it's less likely.\n                        npy_filename = f\"{base_name}_label.npy\"\n                    \n                    np.save(os.path.join(OUTPUT_GT_DIR, npy_filename), frame_level_anomaly)\n                else:\n                    print(f\"Warning: Could not process 'volLabel' for {mat_file} after shape handling. Skipping conversion.\")\n\n            else:\n                print(f\"Warning: 'volLabel' key not found in {mat_file}. Skipping conversion.\")\n\n        except Exception as e:\n            print(f\"ERROR: Failed to convert {mat_file}. Error: {e}\")\n\nprint(\"Conversion complete.\")\nprint(f\"Converted .npy files are saved to: {OUTPUT_GT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:18:42.449570Z","iopub.execute_input":"2025-10-09T13:18:42.449877Z","iopub.status.idle":"2025-10-09T13:18:42.457395Z","shell.execute_reply.started":"2025-10-09T13:18:42.449849Z","shell.execute_reply":"2025-10-09T13:18:42.456579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/3DCAE/train.py\nimport numpy as np\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv3D, Conv3DTranspose, Input, MaxPooling3D, UpSampling3D\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom data_generator import VideoSequenceGenerator # Import our custom generator\nfrom datetime import datetime\n\n# --- Configuration ---\n# Match these with what was used in data_generator.py\nSEQUENCE_LENGTH = 16 # <--- REDUCED SEQUENCE LENGTH (from 16 to 8)\nRESIZE_DIM = (128, 128) # <--- REDUCED IMAGE DIMENSIONS (from 128x128 to 64x64)\nCHANNELS = 3 # Number of color channels (RGB)\nBATCH_SIZE = 2 # <--- REDUCED BATCH SIZE (from 2 to 1)\n\nEPOCHS = 50 # Number of training epochs\nLEARNING_RATE = 0.0001\nMODEL_SAVE_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data/model\" # Directory to save trained models\nMODEL_FILENAME = \"3DCAE_model_to_be_used.h5\" # Name for the saved model\nLOG_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data/logs\" # Directory for TensorBoard logs\n\n# --- Data Paths (from vid2array.py output) ---\nTRAINING_VIDEO_PATHS_FILE = \"/kaggle/working/3DCAE/3DCAE_processed_data/combined_training_video_paths.txt\"\nVALIDATION_VIDEO_PATHS_FILE = \"/kaggle/working/3DCAE/3DCAE_processed_data/combined_validation_video_paths.txt\"\n\n# Ensure model save directory exists\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\nos.makedirs(LOG_DIR, exist_ok=True)\n\n\n# --- Model Architecture (3D CNN Autoencoder) ---\ndef build_autoencoder(input_shape):\n    input_layer = Input(shape=input_shape)\n\n    # Encoder\n    x = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same')(input_layer)\n    x = MaxPooling3D(pool_size=(1, 2, 2), padding='same')(x) # Pool spatially, not temporally\n    \n    x = Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(x) # Pool temporally and spatially\n\n    x = Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(x)\n    \n    encoded = Conv3D(filters=512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x) # Bottleneck\n\n    # Decoder\n    x = Conv3DTranspose(filters=256, kernel_size=(3, 3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling3D(size=(2, 2, 2))(x)\n\n    x = Conv3DTranspose(filters=128, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    x = UpSampling3D(size=(2, 2, 2))(x)\n\n    x = Conv3DTranspose(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    x = UpSampling3D(size=(1, 2, 2))(x) # Upsample spatially, not temporally in first dim\n\n    decoded = Conv3DTranspose(filters=CHANNELS, kernel_size=(3, 3, 3), activation='sigmoid', padding='same')(x) # Output layer\n\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    return autoencoder\n\n# --- Main Training Logic ---\nif __name__ == '__main__':\n    print(\"Setting up model and data generator...\")\n    input_shape = (SEQUENCE_LENGTH, RESIZE_DIM[0], RESIZE_DIM[1], CHANNELS)\n    model = build_autoencoder(input_shape)\n    \n    optimizer = Adam(learning_rate=LEARNING_RATE)\n    model.compile(optimizer=optimizer, loss='mse') # Mean Squared Error for reconstruction loss\n\n    model.summary()\n\n    # --- Data Generators ---\n    # Create the training data generator\n    train_generator = VideoSequenceGenerator(\n        video_paths_file=TRAINING_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH,\n        resize_dim=RESIZE_DIM,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    \n    # Create the validation data generator\n    validation_generator = VideoSequenceGenerator(\n        video_paths_file=VALIDATION_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH,\n        resize_dim=RESIZE_DIM,\n        batch_size=BATCH_SIZE,\n        shuffle=False # Don't shuffle validation data\n    )\n\n    # Callbacks\n    model_checkpoint_callback = ModelCheckpoint(\n        filepath=os.path.join(MODEL_SAVE_DIR, MODEL_FILENAME),\n        monitor='val_loss', # Changed monitor to validation loss\n        save_best_only=True,\n        save_weights_only=False,\n        mode='min',\n        verbose=1\n    )\n\n    early_stopping_callback = EarlyStopping(\n        monitor='val_loss', # Changed monitor to validation loss\n        patience=5,\n        restore_best_weights=True,\n        mode='min',\n        verbose=1\n    )\n\n    reduce_lr_callback = ReduceLROnPlateau(\n        monitor='val_loss', # Changed monitor to validation loss\n        factor=0.5,\n        patience=2,\n        min_lr=0.000001,\n        mode='min',\n        verbose=1\n    )\n    \n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(LOG_DIR, datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n\n\n    print(\"\\nStarting model training...\")\n    history = model.fit(\n        train_generator,\n        validation_data=validation_generator,\n        epochs=EPOCHS,\n        callbacks=[model_checkpoint_callback, early_stopping_callback, reduce_lr_callback, tensorboard_callback],\n        verbose=1\n    )\n\n    print(\"\\nTraining finished.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/3DCAE/train_sma.py\nimport numpy as np\nimport os\nimport cv2\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv3D, Conv3DTranspose, Input, MaxPooling3D, UpSampling3D\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import f1_score\nfrom data_generator import VideoSequenceGenerator\nfrom datetime import datetime\n\n# --- Configuration ---\nSEQUENCE_LENGTH = 16\nRESIZE_DIM = (128, 128)\nCHANNELS = 3\nBATCH_SIZE = 2\nEPOCHS = 50\nLEARNING_RATE = 0.0001  # Default, will be optimized by SMA\nMODEL_SAVE_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data/model\"\nMODEL_FILENAME = \"3DCAE_model_to_be_used.h5\" \nLOG_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data/logs\"\nTRAINING_VIDEO_PATHS_FILE = \"/kaggle/working/3DCAE/3DCAE_processed_data/combined_training_video_paths.txt\"\nVALIDATION_VIDEO_PATHS_FILE = \"/kaggle/working/3DCAE/3DCAE_processed_data/combined_validation_video_paths.txt\"\nTESTING_VIDEO_PATHS_FILE = \"/kaggle/working/3DCAE/3DCAE_processed_data/combined_validation_video_paths.txt\"\nGROUND_TRUTH_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data/ground_truth\"\nANNOTATION_THRESHOLD = 0.0015\n\n# Ensure directories exist\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\nos.makedirs(LOG_DIR, exist_ok=True)\n\n# --- Ground Truth Masks ---\ndef load_ground_truth_masks(gt_base_dir, test_sequence_folder_path, target_resize_dim):\n    masks = []\n    sequence_folder_name = os.path.basename(test_sequence_folder_path)\n    gt_subfolder_name = sequence_folder_name + \"_gt\"\n    gt_folder_path = os.path.join(gt_base_dir, gt_subfolder_name)\n    if not os.path.exists(gt_folder_path):\n        print(f\"Ground truth folder {gt_folder_path} not found, skipping...\")\n        return None\n    mask_filenames = sorted([f for f in os.listdir(gt_folder_path) if f.endswith(('.bmp', '.png', '.jpg'))])\n    for filename in mask_filenames:\n        mask_path = os.path.join(gt_base_dir, gt_subfolder_name, filename)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        if mask is None:\n            continue\n        mask = (mask > 0).astype(np.uint8) * 255\n        if mask.shape != target_resize_dim:\n            mask = cv2.resize(mask, (target_resize_dim[1], target_resize_dim[0]), interpolation=cv2.INTER_NEAREST)\n        if CHANNELS == 3:  # Convert grayscale mask to RGB\n            mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)\n        masks.append(mask)\n    if not masks:\n        print(f\"No valid masks found in {gt_folder_path}, skipping...\")\n        return None\n    return np.array(masks)\n\n# --- Model Architecture ---\ndef build_autoencoder(input_shape, filters_1=64, kernel_size_1=3, filters_2=128, kernel_size_2=3):\n    input_layer = Input(shape=input_shape)\n    kernel_size_1 = (int(kernel_size_1), int(kernel_size_1), int(kernel_size_1))\n    kernel_size_2 = (int(kernel_size_2), int(kernel_size_2), int(kernel_size_2))\n    \n    # Encoder\n    x = Conv3D(filters=int(filters_1), kernel_size=kernel_size_1, activation='relu', padding='same')(input_layer)\n    x = MaxPooling3D(pool_size=(1, 2, 2), padding='same')(x)\n    x = Conv3D(filters=int(filters_2), kernel_size=kernel_size_2, activation='relu', padding='same')(x)\n    x = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(x)\n    x = Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    x = MaxPooling3D(pool_size=(2, 2, 2), padding='same')(x)\n    encoded = Conv3D(filters=512, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    \n    # Decoder\n    x = Conv3DTranspose(filters=256, kernel_size=(3, 3, 3), activation='relu', padding='same')(encoded)\n    x = UpSampling3D(size=(2, 2, 2))(x)\n    x = Conv3DTranspose(filters=128, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    x = UpSampling3D(size=(2, 2, 2))(x)\n    x = Conv3DTranspose(filters=64, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)\n    x = UpSampling3D(size=(1, 2, 2))(x)\n    decoded = Conv3DTranspose(filters=CHANNELS, kernel_size=(3, 3, 3), activation='sigmoid', padding='same')(x)\n    \n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    return autoencoder\n\n# --- Objective Function for SMA ---\ndef objective_function(params):\n    lr, filters_1, kernel_size_1, filters_2, kernel_size_2 = params\n    print(f\"Evaluating params: lr={lr:.6f}, filters_1={int(filters_1)}, kernel_size_1={int(kernel_size_1)}, filters_2={int(filters_2)}, kernel_size_2={int(kernel_size_2)}\")\n    input_shape = (SEQUENCE_LENGTH, RESIZE_DIM[0], RESIZE_DIM[1], CHANNELS)\n    model = build_autoencoder(input_shape, filters_1, kernel_size_1, filters_2, kernel_size_2)\n    optimizer = Adam(learning_rate=lr)\n    model.compile(optimizer=optimizer, loss='mse')\n    \n    train_generator = VideoSequenceGenerator(\n        video_paths_file=TRAINING_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH,\n        resize_dim=RESIZE_DIM,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    validation_generator = VideoSequenceGenerator(\n        video_paths_file=VALIDATION_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH,\n        resize_dim=RESIZE_DIM,\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n    test_generator = VideoSequenceGenerator(\n        video_paths_file=TESTING_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH,\n        resize_dim=RESIZE_DIM,\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n    \n    model_checkpoint_callback = ModelCheckpoint(\n        filepath=os.path.join(MODEL_SAVE_DIR, f\"temp_model_{os.getpid()}.h5\"),\n        monitor='val_loss',\n        save_best_only=True,\n        save_weights_only=False,\n        mode='min',\n        verbose=0\n    )\n    # Using patience=3 for quick evaluation within the objective function\n    early_stopping_callback = EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True,\n        mode='min',\n        verbose=0\n    )\n    reduce_lr_callback = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=2,\n        min_lr=1e-6,\n        mode='min',\n        verbose=0\n    )\n    \n    model.fit(\n        train_generator,\n        epochs=15, # Limited epochs for hyperparameter search\n        validation_data=validation_generator,\n        callbacks=[model_checkpoint_callback, early_stopping_callback, reduce_lr_callback],\n        verbose=0\n    )\n    \n    all_reconstruction_errors = []\n    all_true_labels = []\n    all_test_sequence_folders = test_generator._load_sequence_folder_paths(TESTING_VIDEO_PATHS_FILE)\n    for sequence_folder_path in all_test_sequence_folders:\n        original_frames_resized = test_generator._get_image_frames_from_folder(sequence_folder_path)\n        if not original_frames_resized:\n            continue\n        gt_pixel_masks_for_sequence = load_ground_truth_masks(GROUND_TRUTH_DIR, sequence_folder_path, RESIZE_DIM)\n        sequences_from_current_folder = []\n        if len(original_frames_resized) >= SEQUENCE_LENGTH:\n            for i in range(0, len(original_frames_resized) - SEQUENCE_LENGTH + 1):\n                sequence = original_frames_resized[i: i + SEQUENCE_LENGTH]\n                sequences_from_current_folder.append(np.array(sequence, dtype=np.float32))\n        for seq_idx_in_folder, sequence in enumerate(sequences_from_current_folder):\n            input_batch = np.expand_dims(sequence, axis=0)\n            reconstructed_batch = model.predict(input_batch, verbose=0)\n            reconstruction_error = np.mean(np.square(sequence - reconstructed_batch[0]))\n            middle_frame_idx_in_sequence = SEQUENCE_LENGTH // 2\n            global_frame_idx = seq_idx_in_folder + middle_frame_idx_in_sequence\n            if gt_pixel_masks_for_sequence is not None and global_frame_idx < len(gt_pixel_masks_for_sequence):\n                gt_segment_start = seq_idx_in_folder\n                gt_segment_end = seq_idx_in_folder + SEQUENCE_LENGTH\n                if gt_segment_end <= len(gt_pixel_masks_for_sequence):\n                    sequence_gt_label = np.any(gt_pixel_masks_for_sequence[gt_segment_start: gt_segment_end] > 0).astype(int)\n                    all_reconstruction_errors.append(reconstruction_error)\n                    all_true_labels.append(sequence_gt_label)\n    \n    if all_reconstruction_errors and all_true_labels:\n        all_reconstruction_errors = np.array(all_reconstruction_errors)\n        all_true_labels = np.array(all_true_labels)\n        all_true_labels = (all_true_labels > 0).astype(int)\n        if len(np.unique(all_true_labels)) > 1:\n            predictions = (all_reconstruction_errors > ANNOTATION_THRESHOLD).astype(int)\n            f1 = f1_score(all_true_labels, predictions)\n            # SMA minimizes, so return negative F1\n            return -f1\n    print(\"No valid ground truth data for F1 score, returning high loss...\")\n    return float('inf')\n\n# --- Slime Mould Algorithm ---\ndef slime_mould_algorithm(objective_function, PopulationSize=6, MaxIters=3, dimensions=5, lb=[1e-5, 16, 3, 32, 3], ub=[1e-2, 128, 7, 256, 7], z=0.03):\n    num_agents = PopulationSize\n    max_iters = MaxIters\n    # Set the seed for reproducibility\n    np.random.seed(42) \n    random.seed(42)\n    positions = np.random.uniform(lb, ub, size=(num_agents, dimensions))\n    best_pos = None\n    best_fitness = float('inf')\n\n    for t in range(max_iters):\n        # Calculate fitness for all positions\n        # Note: This is computationally intensive as it trains a model per position per iteration\n        fitness_value = np.array([objective_function(pos) for pos in positions])\n        \n        min_idx = np.argmin(fitness_value)\n        if fitness_value[min_idx] < best_fitness:\n            best_fitness = fitness_value[min_idx]\n            best_pos = positions[min_idx]\n\n        sorted_indices = np.argsort(fitness_value)\n        positions = positions[sorted_indices]\n        fitness_value = fitness_value[sorted_indices]\n        X_b = positions[0] # Best position (leader)\n\n        # Update weights (W) and search parameters (a, vb, vc)\n        # Weight (W) calculation based on fitness difference\n        # Avoid division by zero/very small number\n        max_fit = np.max(fitness_value)\n        min_fit = np.min(fitness_value)\n        \n        # Calculate W array for all agents (used in exploration/exploitation)\n        # The equation for W in the original script is:\n        # w = 1 + random.random() * np.log((fitness_value[0] - fitness_value[j]) / (np.max(fitness_value) - np.min(fitness_value) + 1e-10) + 1)\n        # This seems to be done per agent inside the loop, so we'll keep it there.\n        \n        arctanh_input = np.clip(-(t / max_iters) + 1, -0.999, 0.999)\n        a = np.arctanh(arctanh_input) # Global search parameter 'a'\n\n        for j in range(num_agents):\n            r = random.random()\n            # Probability 'p' of transitioning from exploration to exploitation\n            p = np.tanh(np.abs(fitness_value[j] - best_fitness)) \n            \n            # Local search parameters\n            vb = random.uniform(-a, a)\n            vc = random.uniform(-(1 - t / max_iters), 1 - t / max_iters)\n\n            # Weight calculation for the current agent\n            if max_fit == min_fit:\n                # Handle case where all fitness values are the same\n                w = 1.0\n            else:\n                 # Original weight calculation from the script\n                 w = 1 + random.random() * np.log((fitness_value[0] - fitness_value[j]) / (max_fit - min_fit + 1e-10) + 1)\n\n            # Update position (Movement)\n            if r < z: # Exploration (Random relocation)\n                positions[j] = np.random.uniform(lb, ub, size=dimensions)\n            elif r < p: # Exploitation (Movement towards best position influenced by others)\n                idx_A, idx_B = random.sample(range(num_agents), 2)\n                X_A, X_B = positions[idx_A], positions[idx_B]\n                # Note: The original script uses X_A and X_B as randomly selected positions, \n                # which is common in SMA for simulating vein width influence.\n                positions[j] = X_b + vb * (w * (X_A - X_B))\n            else: # Exploitation (Local search / movement based on current position and 'vc')\n                positions[j] = vc * positions[j]\n\n            # Boundary check and constraint handling\n            positions[j] = np.clip(positions[j], lb, ub)\n            # Ensure odd kernel sizes for stability/consistency\n            positions[j][2] = 2 * round(positions[j][2] / 2) + 1  # Ensure odd kernel_size_1\n            positions[j][4] = 2 * round(positions[j][4] / 2) + 1  # Ensure odd kernel_size_2\n\n        print(f\"Iteration {t+1}/{max_iters} - Best Fitness (-F1): {best_fitness}\")\n\n    return best_pos, best_fitness\n\n# --- Main Training Logic ---\nif __name__ == '__main__':\n    print(\"Running Slime Mould Algorithm for hyperparameter optimization...\")\n    lb = [1e-5, 16, 3, 32, 3]  # [lr, filters_1, kernel_size_1, filters_2, kernel_size_2]\n    ub = [1e-2, 128, 7, 256, 7]\n    best_params, best_fitness = slime_mould_algorithm(\n        objective_function,\n        PopulationSize=6,\n        MaxIters=3,\n        lb=lb,\n        ub=ub\n    )\n    print(f\"Optimal Hyperparameters: lr={best_params[0]:.6f}, filters_1={int(best_params[1])}, \"\n          f\"kernel_size_1={int(best_params[2])}, filters_2={int(best_params[3])}, kernel_size_2={int(best_params[4])}\")\n    print(f\"Best Objective (-F1): {best_fitness} (F1: {-best_fitness:.4f})\")\n\n    print(\"\\nSetting up final model with optimized hyperparameters...\")\n    input_shape = (SEQUENCE_LENGTH, RESIZE_DIM[0], RESIZE_DIM[1], CHANNELS)\n    model = build_autoencoder(best_params[1], best_params[2], best_params[3], best_params[4]) # Pass params to build_autoencoder\n    \n    optimizer = Adam(learning_rate=best_params[0])\n    model.compile(optimizer=optimizer, loss='mse')\n    model.summary()\n\n    # --- Data Generators ---\n    train_generator = VideoSequenceGenerator(\n        video_paths_file=TRAINING_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH,\n        resize_dim=RESIZE_DIM,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    validation_generator = VideoSequenceGenerator(\n        video_paths_file=VALIDATION_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH,\n        resize_dim=RESIZE_DIM,\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n\n    # --- Callbacks ---\n    model_checkpoint_callback = ModelCheckpoint(\n        filepath=os.path.join(MODEL_SAVE_DIR, MODEL_FILENAME),\n        monitor='val_loss',\n        save_best_only=True,\n        save_weights_only=False,\n        mode='min',\n        verbose=1\n    )\n    # INTEGRATED: EarlyStopping patience = 5\n    early_stopping_callback = EarlyStopping(\n        monitor='val_loss',\n        patience=5, \n        restore_best_weights=True,\n        mode='min',\n        verbose=1\n    )\n    # INTEGRATED: ReduceLROnPlateau min_lr = 0.000001\n    reduce_lr_callback = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=2,\n        min_lr=0.000001, \n        mode='min',\n        verbose=1\n    )\n    # Ensure TensorBoard uses tf.keras.callbacks.TensorBoard for compatibility\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(LOG_DIR, datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n\n    print(\"\\nStarting final model training...\")\n    history = model.fit(\n        train_generator,\n        validation_data=validation_generator,\n        epochs=EPOCHS,\n        callbacks=[model_checkpoint_callback, early_stopping_callback, reduce_lr_callback, tensorboard_callback],\n        verbose=1\n    )\n\n    print(\"\\nTraining finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/3DCAE/test.py\nimport numpy as np\nimport os\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom data_generator import VideoSequenceGenerator\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n# --- Configuration ---\n# IMPORTANT: These MUST match what you successfully used for training in train.py and data_generator.py\nSEQUENCE_LENGTH = 16 # <--- Matches the trained model's sequence length\nRESIZE_DIM = (128, 128) # <--- Matches the trained model's resize dimensions\nCHANNELS = 3\nBATCH_SIZE = 2 # Use a batch size of 1 for testing to process one sequence at a time for anomaly scoring\n\n# --- Paths ---\nMODEL_PATH = \"/kaggle/input/3dcae-model-project/tensorflow1/default/1/3DCAE_model_to_be_used.h5\"\nTESTING_VIDEO_PATHS_FILE = \"/kaggle/working/3DCAE/3DCAE_processed_data/combined_testing_video_paths.txt\"\nAVENUE_GROUND_TRUTH_VOL = \"/kaggle/working/3DCAE/3DCAE_processed_data/combined_testing_label_mask_npy\" \nRESULTS_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data/results\"\n\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n# --- Function to load ground truth ---\ndef load_ground_truth(ground_truth_path, video_path):\n    video_id_str = os.path.basename(video_path).split('.')[0]\n    video_id_for_gt = str(int(video_id_str)) \n    ground_truth_mask_file = os.path.join(ground_truth_path, f\"{video_id_for_gt}_label.npy\") \n    \n    try:\n        gt_labels_for_frames = np.load(ground_truth_mask_file)\n        return gt_labels_for_frames\n    except FileNotFoundError:\n        print(f\"Warning (GT File Not Found): {ground_truth_mask_file}. Video: {video_path}. Skipping for AUC.\")\n        return None \n    except Exception as e:\n        print(f\"Warning (GT Loading Error): {ground_truth_mask_file}. Error: {e}. Video: {video_path}. Skipping for AUC.\")\n        return None\n\n# --- Main Evaluation Logic ---\nif __name__ == '__main__':\n    print(\"Loading trained model...\")\n    try:\n        model = load_model(MODEL_PATH, custom_objects={'mse': tf.keras.losses.MeanSquaredError()})\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load model from {MODEL_PATH}. Error: {e}\")\n        print(\"Please ensure training completed and the model file exists.\")\n        exit(1)\n\n    print(\"Setting up test data generator...\")\n    test_generator = VideoSequenceGenerator(\n        video_paths_file=TESTING_VIDEO_PATHS_FILE,\n        sequence_length=SEQUENCE_LENGTH, # <--- ENSURE THIS MATCHES\n        resize_dim=RESIZE_DIM, # <--- ENSURE THIS MATCHES\n        batch_size=BATCH_SIZE, \n        shuffle=False \n    )\n    \n    video_paths_for_testing = test_generator._load_video_paths(TESTING_VIDEO_PATHS_FILE)\n    print(f\"DEBUG TEST: Total test video paths loaded: {len(video_paths_for_testing)}\")\n    if len(video_paths_for_testing) > 0:\n        print(f\"DEBUG TEST: First test video path: {video_paths_for_testing[0]}\")\n    else:\n        print(\"DEBUG TEST: No test video paths found in the list.\")\n\n\n    all_reconstruction_errors = []\n    all_true_labels = [] \n\n    print(\"\\nStarting evaluation on test videos...\")\n    \n    if not video_paths_for_testing:\n        print(\"ERROR: No test video paths available to evaluate. Exiting test script.\")\n        exit(1)\n\n    for video_path in tqdm(video_paths_for_testing, desc=\"Evaluating Test Videos\"):\n        gt_labels_for_frames = load_ground_truth(AVENUE_GROUND_TRUTH_VOL, video_path)\n        \n        sequences_from_current_video = test_generator._get_video_sequences(video_path)\n        \n        if not sequences_from_current_video:\n            continue\n\n        video_sequence_errors = []\n        for sequence in sequences_from_current_video:\n            input_batch = np.expand_dims(sequence, axis=0)\n            reconstructed_batch = model.predict(input_batch, verbose=0)\n            \n            reconstruction_error = np.mean(np.square(sequence - reconstructed_batch[0]))\n            video_sequence_errors.append(reconstruction_error)\n\n        if gt_labels_for_frames is not None and len(gt_labels_for_frames) >= SEQUENCE_LENGTH:\n            gt_labels_for_sequences = []\n            for i in range(len(video_sequence_errors)): \n                if (i + SEQUENCE_LENGTH) <= len(gt_labels_for_frames):\n                    if np.any(gt_labels_for_frames[i : i + SEQUENCE_LENGTH] > 0):\n                        gt_labels_for_sequences.append(1)\n                    else:\n                        gt_labels_for_sequences.append(0)\n                else:\n                    gt_labels_for_sequences.append(0)\n            \n            if len(gt_labels_for_sequences) == len(video_sequence_errors):\n                all_reconstruction_errors.extend(video_sequence_errors)\n                all_true_labels.extend(gt_labels_for_sequences)\n            else:\n                print(f\"Warning: Mismatch between sequences ({len(video_sequence_errors)}) and GT labels ({len(gt_labels_for_sequences)}) for {video_path}. Skipping for AUC.\")\n        else:\n            print(f\"Warning: No valid ground truth to match sequences for {video_path}. Skipping for AUC.\")\n\n    if not all_reconstruction_errors or not all_true_labels:\n        print(\"No valid test sequences with matching ground truth were processed. Cannot calculate AUC metrics.\")\n    else:\n        all_reconstruction_errors = np.array(all_reconstruction_errors)\n        all_true_labels = np.array(all_true_labels)\n        \n        all_true_labels = (all_true_labels > 0).astype(int)\n        \n        # --- Save results to .npy files ---\n        results_save_path = os.path.join(RESULTS_DIR, \"reconstruction_errors.npy\")\n        labels_save_path = os.path.join(RESULTS_DIR, \"true_labels.npy\")\n        np.save(results_save_path, all_reconstruction_errors)\n        np.save(labels_save_path, all_true_labels)\n        print(f\"Reconstruction errors saved to: {results_save_path}\")\n        print(f\"True labels saved to: {labels_save_path}\")\n\n        if len(np.unique(all_true_labels)) > 1: \n            fpr, tpr, thresholds = roc_curve(all_true_labels, all_reconstruction_errors)\n            roc_auc = auc(fpr, tpr)\n\n            print(f\"\\nEvaluation Complete.\")\n            print(f\"Total sequences processed: {len(all_reconstruction_errors)}\")\n            print(f\"Total anomalous sequences: {np.sum(all_true_labels)}\")\n            print(f\"Anomaly Detection AUC: {roc_auc:.4f}\")\n\n            # --- Metrics at a fixed threshold (now for reference, will be interactive in notebook) ---\n            # FIXED_THRESHOLD = 0.015 # This is just a placeholder here, actual tuning done interactively\n            # y_pred = (all_reconstruction_errors > FIXED_THRESHOLD).astype(int)\n            # acc = accuracy_score(all_true_labels, y_pred, zero_division=0) # zero_division handles cases where precision/recall is 0\n            # prec = precision_score(all_true_labels, y_pred, zero_division=0)\n            # rec = recall_score(all_true_labels, y_pred, zero_division=0)\n            # f1 = f1_score(all_true_labels, y_pred, zero_division=0)\n            # print(f\"\\n(Example) Metrics at Threshold = {FIXED_THRESHOLD:.4f}:\")\n            # print(f\"  Accuracy:  {acc:.4f}\")\n            # print(f\"  Precision: {prec:.4f}\")\n            # print(f\"  Recall:    {rec:.4f}\")\n            # print(f\"  F1-Score:  {f1:.4f}\")\n\n            plt.figure()\n            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n            plt.xlim([0.0, 1.0])\n            plt.ylim([0.0, 1.05])\n            plt.xlabel('False Positive Rate')\n            plt.ylabel('True Positive Rate')\n            plt.title('Receiver Operating Characteristic (ROC) Curve')\n            plt.legend(loc=\"lower right\")\n            plt.grid(True)\n            plt.savefig(os.path.join(RESULTS_DIR, \"roc_curve.png\"))\n            plt.show()\n            print(f\"ROC curve saved to {os.path.join(RESULTS_DIR, 'roc_curve.png')}\")\n        else:\n            print(\"Warning: Only one class (normal or anomalous) found in true labels. Cannot calculate full metrics.\")\n            print(f\"Total sequences processed: {len(all_reconstruction_errors)}\")\n            print(f\"Total anomalous sequences: {np.sum(all_true_labels)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:21:05.696431Z","iopub.execute_input":"2025-10-09T13:21:05.697281Z","iopub.status.idle":"2025-10-09T13:21:05.706036Z","shell.execute_reply.started":"2025-10-09T13:21:05.697248Z","shell.execute_reply":"2025-10-09T13:21:05.705356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/3DCAE/video_visual.py\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport collections\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display, HTML\nimport shutil\n\n# --- Configuration ---\n# IMPORTANT: These MUST match your trained model\nMODEL_PATH = \"/kaggle/working/3DCAE/3DCAE_processed_data/model/3DCAE_model_to_be_used.h5\"\nSEQUENCE_LENGTH = 16\nRESIZE_DIM = (128, 128)\nCHANNELS = 3\n# A placeholder threshold. You MUST tune this value based on your specific video data.\nANOMALY_THRESHOLD = 0.015 \n# Placeholder for your video file.\nVIDEO_SOURCE = \"/kaggle/input/futminna-dataset/testing_videos/testing_videos/19.avi\"\n# New output path for the video file\nOUTPUT_VIDEO_PATH = \"anomaly_visualization_army_fut_testing_19.mp4\"\n# Limit the number of frames to process for notebook display\nMAX_FRAMES_TO_PROCESS = 450\n\n# --- Main Logic ---\ndef notebook_visualizer():\n    \"\"\"\n    Simulates a live CCTV feed, detects anomalies, and displays output in a notebook.\n    \"\"\"\n    print(\"Loading trained autoencoder model...\")\n    try:\n        model = load_model(MODEL_PATH, custom_objects={'mse': tf.keras.losses.MeanSquaredError()})\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load model from {MODEL_PATH}. Please ensure it exists.\")\n        print(f\"Error details: {e}\")\n        return\n\n    print(\"Opening video source...\")\n    cap = cv2.VideoCapture(VIDEO_SOURCE)\n\n    if not cap.isOpened():\n        print(f\"ERROR: Could not open video source {VIDEO_SOURCE}\")\n        return\n\n    # Get video properties for the output video writer\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Define the codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Use 'mp4v' for .mp4 format\n    video_writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n\n    sequence_buffer = collections.deque(maxlen=SEQUENCE_LENGTH)\n    frame_count = 0\n    all_reconstruction_errors = []\n\n    print(f\"\\nStarting anomaly visualization. Processing frames and writing to {OUTPUT_VIDEO_PATH}...\")\n\n    try:\n        while frame_count < MAX_FRAMES_TO_PROCESS:\n            ret, frame = cap.read()\n            if not ret:\n                print(\"End of video or read error.\")\n                break\n\n            processed_frame = cv2.resize(frame, RESIZE_DIM, interpolation=cv2.INTER_AREA)\n            processed_frame = processed_frame / 255.0  # Normalize to [0, 1]\n            \n            sequence_buffer.append(processed_frame)\n\n            if len(sequence_buffer) == SEQUENCE_LENGTH:\n                input_sequence = np.expand_dims(np.array(list(sequence_buffer)), axis=0)\n                reconstructed_sequence = model.predict(input_sequence, verbose=0)\n                \n                original_last_frame = input_sequence[0, -1]\n                reconstructed_last_frame = reconstructed_sequence[0, -1]\n                \n                reconstruction_error = np.mean(np.square(original_last_frame - reconstructed_last_frame))\n                all_reconstruction_errors.append(reconstruction_error)\n                \n                display_frame = frame.copy()\n                text = f\"Score: {reconstruction_error:.4f}\"\n                \n                if reconstruction_error > ANOMALY_THRESHOLD:\n                    text_color = (0, 0, 255)  # Red (BGR)\n                    alert_text = \"ANOMALY DETECTED!\"\n                    cv2.putText(display_frame, alert_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n                else:\n                    text_color = (0, 255, 0)  # Green (BGR)\n\n                cv2.putText(display_frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2, cv2.LINE_AA)\n                \n                # Write the frame to the output video file\n                video_writer.write(display_frame)\n\n            frame_count += 1\n            print(f\"Processed frame {frame_count}/{MAX_FRAMES_TO_PROCESS}\", end='\\r')\n\n        print(\"\\nFinished processing frames. Displaying results...\")\n\n    finally:\n        cap.release()\n        video_writer.release()\n        print(f\"Video saved to {OUTPUT_VIDEO_PATH}\")\n\n    # --- Plot reconstruction error over time ---\n    plt.figure(figsize=(12, 6))\n    plt.plot(all_reconstruction_errors, label='Reconstruction Error')\n    plt.axhline(y=ANOMALY_THRESHOLD, color='r', linestyle='--', label=f'Anomaly Threshold ({ANOMALY_THRESHOLD})')\n    plt.xlabel('Frame')\n    plt.ylabel('Error')\n    plt.title('Reconstruction Error Over Time')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nif __name__ == '__main__':\n    notebook_visualizer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:11:30.952519Z","iopub.execute_input":"2025-10-09T13:11:30.953077Z","iopub.status.idle":"2025-10-09T13:11:30.960164Z","shell.execute_reply.started":"2025-10-09T13:11:30.953055Z","shell.execute_reply":"2025-10-09T13:11:30.959368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell for .mat to .npy conversion (run this!)\nimport os\nimport scipy.io\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nprint(\"Starting conversion of .mat ground truth files to .npy...\")\n\nINPUT_GT_DIR = \"/kaggle/input/avenuedataset/Avenuedataset/ground_truth_demo/ground_truth_demo/testing_label_mask\"\nOUTPUT_PROCESSED_DATA_DIR = \"/kaggle/working/3DCAE/3DCAE_processed_data\"\nOUTPUT_GT_DIR = os.path.join(OUTPUT_PROCESSED_DATA_DIR, \"combined_testing_label_mask_npy\")\nos.makedirs(OUTPUT_GT_DIR, exist_ok=True)\n\nmat_files = [f for f in os.listdir(INPUT_GT_DIR) if f.endswith(\".mat\")]\n\nif not mat_files:\n    print(f\"No .mat files found in {INPUT_GT_DIR}. Please check the ground truth path.\")\nelse:\n    for mat_file in tqdm(mat_files, desc=\"Converting .mat to .npy\"):\n        mat_path = os.path.join(INPUT_GT_DIR, mat_file)\n        \n        try:\n            mat_data = scipy.io.loadmat(mat_path)\n            \n            if 'volLabel' in mat_data: \n                mask_data_raw = mat_data['volLabel'] \n                \n                frame_level_anomaly = None\n                \n                if mask_data_raw.ndim == 2 and mask_data_raw.shape[0] == 1 and mask_data_raw.dtype == object:\n                    list_of_frame_masks = mask_data_raw[0]\n                    frame_level_anomaly = np.array([1 if np.any(frame_mask > 0) else 0 for frame_mask in list_of_frame_masks]).astype(int)\n                elif mask_data_raw.ndim == 1:\n                    frame_level_anomaly = (mask_data_raw.astype(float) > 0).astype(int)\n                elif mask_data_raw.ndim == 3:\n                    frame_level_anomaly = np.array([1 if np.any(mask_data_raw[:, :, i].astype(float) > 0) else 0 for i in range(mask_data_raw.shape[2])])\n                else:\n                    print(f\"Warning: Unexpected 'volLabel' shape or dtype in {mat_file}: {mask_data_raw.shape}, {mask_data_raw.dtype}. Skipping conversion.\")\n                    continue \n                    \n                if frame_level_anomaly is not None:\n                    # **FIX IS HERE: Removed the :02d format specifier**\n                    base_name = os.path.splitext(mat_file)[0]\n                    try:\n                        # Extract the number (e.g., '04' -> 4)\n                        video_id = int(base_name.split('_')[0]) \n                        # Format as non-padded string (e.g., 4 -> '4_label.npy')\n                        npy_filename = f\"{video_id}_label.npy\" \n                    except (ValueError, IndexError):\n                        # Fallback\n                        npy_filename = f\"{base_name}_label.npy\"\n                    \n                    np.save(os.path.join(OUTPUT_GT_DIR, npy_filename), frame_level_anomaly)\n                else:\n                    print(f\"Warning: Could not process 'volLabel' for {mat_file} after shape handling. Skipping conversion.\")\n\n            else:\n                print(f\"Warning: 'volLabel' key not found in {mat_file}. Skipping conversion.\")\n\n        except Exception as e:\n            print(f\"ERROR: Failed to convert {mat_file}. Error: {e}\")\n\nprint(\"Conversion complete.\")\nprint(f\"Converted .npy files are saved to: {OUTPUT_GT_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:29:25.765424Z","iopub.execute_input":"2025-10-09T13:29:25.766292Z","iopub.status.idle":"2025-10-09T13:29:37.997221Z","shell.execute_reply.started":"2025-10-09T13:29:25.766260Z","shell.execute_reply":"2025-10-09T13:29:37.996371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nimport collections\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display, HTML\nimport shutil\n\n# --- Configuration ---\n# IMPORTANT: These MUST match your trained model\nMODEL_PATH = \"/kaggle/input/3dcae-model-project/tensorflow1/default/1/3DCAE_model_to_be_used.h5\"\nSEQUENCE_LENGTH = 16\nRESIZE_DIM = (128, 128)\nCHANNELS = 3\n# A placeholder threshold. You MUST tune this value based on your specific video data.\nANOMALY_THRESHOLD = 0.015 \n# Placeholder for your video file.\nVIDEO_SOURCE = \"/kaggle/input/bandit-dataset/VID-20250917-WA0008.mp4\"\n# New output path for the video file\nOUTPUT_VIDEO_PATH = \"ave2.mp4\"\n# Limit the number of frames to process for notebook display\nMAX_FRAMES_TO_PROCESS = 450\n\n# --- Main Logic ---\ndef notebook_visualizer():\n    \"\"\"\n    Simulates a live CCTV feed, detects anomalies, and displays output in a notebook.\n    \"\"\"\n    print(\"Loading trained autoencoder model...\")\n    try:\n        model = load_model(MODEL_PATH, custom_objects={'mse': tf.keras.losses.MeanSquaredError()})\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"ERROR: Failed to load model from {MODEL_PATH}. Please ensure it exists.\")\n        print(f\"Error details: {e}\")\n        return\n\n    print(\"Opening video source...\")\n    cap = cv2.VideoCapture(VIDEO_SOURCE)\n\n    if not cap.isOpened():\n        print(f\"ERROR: Could not open video source {VIDEO_SOURCE}\")\n        return\n\n    # Get video properties for the output video writer\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Define the codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Use 'mp4v' for .mp4 format\n    video_writer = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, fps, (width, height))\n\n    sequence_buffer = collections.deque(maxlen=SEQUENCE_LENGTH)\n    frame_count = 0\n    all_reconstruction_errors = []\n\n    print(f\"\\nStarting anomaly visualization. Processing frames and writing to {OUTPUT_VIDEO_PATH}...\")\n\n    try:\n        while frame_count < MAX_FRAMES_TO_PROCESS:\n            ret, frame = cap.read()\n            if not ret:\n                print(\"End of video or read error.\")\n                break\n\n            processed_frame = cv2.resize(frame, RESIZE_DIM, interpolation=cv2.INTER_AREA)\n            processed_frame = processed_frame / 255.0  # Normalize to [0, 1]\n            \n            sequence_buffer.append(processed_frame)\n\n            if len(sequence_buffer) == SEQUENCE_LENGTH:\n                input_sequence = np.expand_dims(np.array(list(sequence_buffer)), axis=0)\n                reconstructed_sequence = model.predict(input_sequence, verbose=0)\n                \n                original_last_frame = input_sequence[0, -1]\n                reconstructed_last_frame = reconstructed_sequence[0, -1]\n                \n                reconstruction_error = np.mean(np.square(original_last_frame - reconstructed_last_frame))\n                all_reconstruction_errors.append(reconstruction_error)\n                \n                display_frame = frame.copy()\n                text = f\"Score: {reconstruction_error:.4f}\"\n                \n                if reconstruction_error > ANOMALY_THRESHOLD:\n                    text_color = (0, 0, 255)  # Red (BGR)\n                    alert_text = \"ANOMALY DETECTED!\"\n                    cv2.putText(display_frame, alert_text, (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n                else:\n                    text_color = (0, 255, 0)  # Green (BGR)\n\n                cv2.putText(display_frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, text_color, 2, cv2.LINE_AA)\n                \n                # Write the frame to the output video file\n                video_writer.write(display_frame)\n\n            frame_count += 1\n            print(f\"Processed frame {frame_count}/{MAX_FRAMES_TO_PROCESS}\", end='\\r')\n\n        print(\"\\nFinished processing frames. Displaying results...\")\n\n    finally:\n        cap.release()\n        video_writer.release()\n        print(f\"Video saved to {OUTPUT_VIDEO_PATH}\")\n\n    # --- Plot reconstruction error over time ---\n    plt.figure(figsize=(12, 6))\n    plt.plot(all_reconstruction_errors, label='Reconstruction Error')\n    plt.axhline(y=ANOMALY_THRESHOLD, color='r', linestyle='--', label=f'Anomaly Threshold ({ANOMALY_THRESHOLD})')\n    plt.xlabel('Frame')\n    plt.ylabel('Error')\n    plt.title('Reconstruction Error Over Time')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nif __name__ == '__main__':\n    notebook_visualizer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T13:11:55.700150Z","iopub.execute_input":"2025-10-09T13:11:55.700857Z","iopub.status.idle":"2025-10-09T13:13:05.563010Z","shell.execute_reply.started":"2025-10-09T13:11:55.700833Z","shell.execute_reply":"2025-10-09T13:13:05.562156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tf2onnx\n!pip install onnx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T03:49:32.784121Z","iopub.execute_input":"2025-10-04T03:49:32.784372Z","iopub.status.idle":"2025-10-04T03:49:53.717881Z","shell.execute_reply.started":"2025-10-04T03:49:32.784348Z","shell.execute_reply":"2025-10-04T03:49:53.716818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Reuse the loaded model from before (or reload if needed)\nmodel = tf.keras.models.load_model(\n    '/kaggle/input/3dcae-model-project/tensorflow1/default/1/3DCAE_model_to_be_used.h5',\n    compile=False\n)\nmodel.compile(optimizer='adam', loss='mse')\n\nprint(\"Model ready for Flex-enabled conversion...\")\n\n# Converter with Flex (SELECT_TF_OPS) for MaxPool3D fallback\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # Native ops where possible\n    tf.lite.OpsSet.SELECT_TF_OPS     # Flex for tf.MaxPool3D\n]\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantize for speed\nconverter.allow_custom_ops = True  # Required for Flex\n\ntry:\n    tflite_model = converter.convert()\n    print(\" Flex TFLite conversion successful!\")\n    \n    # Save\n    with open('/kaggle/working/new_3DCAE_model.tflite', 'wb') as f:\n        f.write(tflite_model)\n    \n    print(f\"Model size: {len(tflite_model) / (1024*1024):.1f} MB\")\n    \n    # Quick inference test (dummy input)\n    interpreter = tf.lite.Interpreter(model_path='/kaggle/working/new_3DCAE_model.tflite')\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    \n    # Dummy batch=1 input (normalize if your data needs it)\n    dummy_input = tf.random.normal([1, 16, 128, 128, 3]).numpy()\n    interpreter.set_tensor(input_details[0]['index'], dummy_input)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n    print(f\" Dummy inference works! Output shape: {output.shape}, min/max: {output.min():.3f}/{output.max():.3f}\")\n    \nexcept Exception as e:\n    print(f\" Still failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T10:41:38.943432Z","iopub.execute_input":"2025-10-05T10:41:38.944000Z","iopub.status.idle":"2025-10-05T10:41:48.537389Z","shell.execute_reply.started":"2025-10-05T10:41:38.943978Z","shell.execute_reply":"2025-10-05T10:41:48.536540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# Test the actual saved model\ninterpreter = tf.lite.Interpreter(model_path='/kaggle/working/new_3DCAE_model.tflite')\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nprint(f\"Input shape: {input_details[0]['shape']}, dtype: {input_details[0]['dtype']}\")\nprint(f\"Output shape: {output_details[0]['shape']}, dtype: {output_details[0]['dtype']}\")\n\n# Dummy batch=1 input (random normal, but scale to [0,1] if your data is images)\ndummy_input = np.random.uniform(0, 1, [1, 16, 128, 128, 3]).astype(np.float32)  # Better for sigmoid output\ninterpreter.set_tensor(input_details[0]['index'], dummy_input)\ninterpreter.invoke()\noutput = interpreter.get_tensor(output_details[0]['index'])\n\nprint(f\" Dummy inference works! Output shape: {output.shape}, min/max: {output.min():.3f}/{output.max():.3f}\")\nprint(f\"Sample MSE (anomaly score): {np.mean((dummy_input - output)**2):.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T10:41:55.763979Z","iopub.execute_input":"2025-10-05T10:41:55.764284Z","iopub.status.idle":"2025-10-05T10:41:59.966970Z","shell.execute_reply.started":"2025-10-05T10:41:55.764257Z","shell.execute_reply":"2025-10-05T10:41:59.966173Z"}},"outputs":[],"execution_count":null}]}